{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\D}[2]{\\frac{\\partial #1}{\\partial #2}}$ $\\newcommand{\\d}[2]{\\partial #1/\\partial #2}$\n",
    "$\\newcommand{\\DD}[2]{\\frac{\\partial^2 #1}{\\partial {#2}^2}}$ \n",
    "$\\newcommand{\\dd}[2]{\\partial^2 #1/\\partial {#2}^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $L(\\beta)$ be the log-likelihood of the data $X, y$ as a function of the parameters $\\beta$. \n",
    "\n",
    "The score (gradient) is the vector \n",
    "$$s = \\D{L}{\\beta}$$ \n",
    "with elements $s_i = \\partial L/\\partial \\beta_i$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the likelihood is a function of an observation vector $y$ and a mean which is a function of the parameters, $\\mu(\\beta)$, thus\n",
    "\n",
    "$$\n",
    "L(\\beta) = L(y, \\mu(\\beta))\n",
    "$$\n",
    "\n",
    "Then, using the chain rule, \n",
    "\n",
    "$$\n",
    "s = \\D{L}{\\beta} = \\D{L}{\\mu}\\D{\\mu}{\\beta}\n",
    "$$\n",
    "\n",
    "\n",
    "The term $\\d{\\mu}{\\beta}$ is the jacobian $J$ of the predictor $\\mu(\\beta)$ with rows equal to the number of \n",
    "observations $n$ and columns equal to the number of parameters $b$. \n",
    "The term $\\d{L}{\\mu}$ is the gradient of the log-likelihood with respect to the expected value, and has rows equal to the\n",
    "number of observations. This seems to be a normalized deviate. Thus\n",
    "\n",
    "$$\n",
    "s = \\D{L}{\\mu} J\n",
    "$$\n",
    "\n",
    "The einsum signature is `i,ij->j` where $\\d{L}{\\mu}$ and $\\mu$ is indexed by i and $J$ by i & j ($\\beta$ is indexed by j). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hessian is\n",
    "\n",
    "$$\n",
    "H = \\DD{L}{\\beta}=\\DD{L}{\\mu}\\DD{\\mu}{\\beta}\n",
    "$$\n",
    "\n",
    "The expected value of this is\n",
    "\n",
    "$$\n",
    "E(H) = E\\left(\\DD{L}{\\beta}\\right) = E\\left(\\D{L}{\\beta}\\D{L}{\\beta^*} \\right)\n",
    "$$\n",
    "\n",
    "(which needs proving) in which $\\beta$ and $\\beta^*$ are the same vector, iterated over differently. We then have\n",
    "\n",
    "$$\n",
    "E\\left(\\D{L}{\\beta}\\D{L}{\\beta^*} \\right) = \n",
    "E\\left(\\D{L}{\\mu}\\D{\\mu}{\\beta}\\D{L}{\\mu^*}\\D{\\mu^*}{\\beta^*}\\right) =\n",
    "E\\left(\\D{L}{\\mu}\\D{L}{\\mu^*}\\right)\\D{\\mu}{\\beta}\\D{\\mu^*}{\\beta^*}\n",
    "$$\n",
    "\n",
    "The term in the expectation is an outer product since it's over every combination of elements from $\\mu$ and $\\mu^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only nonzero elements of $E\\left(\\d{L}{\\mu}\\,\\d{L}{\\mu*}\\right)$ are when $\\mu=\\mu^*$, because the observations are independent, so this is a diagonal matrix, call it $V$, with elements\n",
    "\n",
    "$$\n",
    "V_{i,i} = E\\left(\\D{L_i}{\\mu_i}\\right)^2\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "E(H) = J'VJ\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "up = os.path.normpath(os.path.join(os.getcwd(), \"../src\"))\n",
    "sys.path.append(up)\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Approach.\n",
    "\n",
    "This is trickier because the jacobian of the mu function must be computed using forward differentiation, for efficiency, and unfortunately that doesn't happen because the jacobian of the mu function has been traced & backtraced. This happens because the derivatives of components of a function are computed prior to the function itself, due to the pullback algorithm:\n",
    "\n",
    "```\n",
    "derivs = []  # holds dY/dZi\n",
    "if True:  # id(n) not in deriv_memo:\n",
    "    for Zi, argno in A.root.backrefs[A]:\n",
    "        derivs.append(\n",
    "            pullback(Zi, argno)\n",
    "        )  # this is dY/dZi*dZi/dA,argno\n",
    "    derivs = summate(derivs)\n",
    "return derivs\n",
    "```\n",
    "\n",
    "So the predictor has to be packaged into the error model to give a function of data & beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxmf.autodiff.tracing import notrace, value_of\n",
    "import maxmf.autodiff.forward as fwd\n",
    "import maxmf.autodiff.reverse as rev\n",
    "\n",
    "def GLM_Builder(score, dLdmu, varfunc):\n",
    "    # creates a GLM likelihood function; this uses the -Fisher instead of the hessian.\n",
    "    # dLdmu is a vector of the derivatives of the likelihood with respect to the expected value\n",
    "    # varfunc is a vector of the expected value of dLdmu**2\n",
    "    \n",
    "    def GLM(meanfunc):\n",
    "        # factory function to create likelihoods with derivatives\n",
    "        # y is the data and meanfunc is a callable which takes (X,beta)\n",
    "        # returns a function L(y,X,beta) which is notraced\n",
    "        \n",
    "        @notrace\n",
    "        def L(y, X, b):\n",
    "            return score(y, meanfunc(X,b))\n",
    "\n",
    "        # reverse mode\n",
    "        \n",
    "        @rev.register_deriv(L)\n",
    "        def dLrev(dYdZ, argno, y, X, b):\n",
    "            # function to work out the meanfunc & jacobian then pass onto\n",
    "            # the actual derivative. This step is necessary to memoize the\n",
    "            # value of mu and J\n",
    "            if argno!=2:\n",
    "                return 0\n",
    "            mfunc = fwd.Diff(meanfunc, argno=1)\n",
    "            mu = mfunc.trace(X, value_of(b))\n",
    "            J = mfunc.jacobian(gc=True)\n",
    "            # we have to keep b as a parameter here so that it will be traced when\n",
    "            # computing higher derivatives.\n",
    "            # NB dydz is guaranteed to be a scalar\n",
    "            return dYdZ*_dLrev(y, mu, J, b)        \n",
    "        \n",
    "        @notrace\n",
    "        def _dLrev(y, mu, J, b):\n",
    "            # works out the derivative of L given mu=meanfunc(X,b) & its jacobian\n",
    "            return np.einsum('i,i...->...', dLdmu(y,mu), J)\n",
    "\n",
    "        @rev.register_deriv(_dLrev)\n",
    "        def dL2rev(dYdZ, argno, y, mu, J, b):\n",
    "            # this doesn't depend on b so has no higher derivatives\n",
    "            # works out the hessian of L given y, mu=meanfunc(X,b) and its jacobian\n",
    "            if argno!=3:\n",
    "                return 0\n",
    "            # we aren't going to do any higher derivatives, so just return\n",
    "            # a matrix\n",
    "            # if J has more than 2 dim, we need to allow for this.\n",
    "            return -dYdZ@np.einsum('i,ij,ik->jk',varfunc(y,mu),J,J)\n",
    "\n",
    "        # forward mode\n",
    "        \n",
    "        @fwd.register_deriv(L)\n",
    "        def dLfwd(y, X, b):\n",
    "            mfunc = fwd.Diff(meanfunc, argno=1)\n",
    "            mu = mfunc.trace(X, value_of(b))\n",
    "            J = mfunc.jacobian(gc=True)\n",
    "            return _dLfwd(y, mu, J, b)\n",
    "\n",
    "        @notrace\n",
    "        def _dLfwd(y, mu, J, b):\n",
    "            # works out the derivative of L given mu=meanfunc(X,b) & its jacobian\n",
    "            return np.einsum('i,i...->...', dLdmu(y,mu), J)\n",
    "        \n",
    "        @fwd.register_deriv(_dLfwd)\n",
    "        def dL2fwd(y, mu, J, b):\n",
    "            # this doesn't depend on b so has no higher derivatives\n",
    "            return -np.einsum('i,ij,ik->jk',varfunc(y,mu),J,J)\n",
    "\n",
    "        return L\n",
    "    \n",
    "    return GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Errors.\n",
    "\n",
    "If the observation $y$ is gaussian with mean $\\mu$, the likelihood is\n",
    "\n",
    "$$\n",
    "Pr(y|\\mu) = C exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The log likelihood is\n",
    "\n",
    "$$L = -\\frac{(y-\\mu)^2}{2\\sigma^2} + \\log{C}$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\D{L}{\\mu} = \\frac{y-\\mu}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "E\\left(\\D{L}{\\mu}\\right)^2 = \\frac{E(y-\\mu)^2}{\\sigma^4}=\\frac{1}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "In this case, because the log-likelihood, the score and $E(H)$ are all multiplied by the same factor $1/\\sigma^2$, we can ignore it, and have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L &= -0.5(y-\\mu)^2 \\\\\n",
    "\\D{L}{\\mu} &= y-\\mu \\\\\n",
    "E\\left(\\D{L}{\\mu}\\right)^2 &= 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian = GLM_Builder(lambda y,mu: -0.5*np.sum((y-mu)**2), \n",
    "                       lambda y,mu: y-mu, \n",
    "                       lambda y,mu: np.ones(y.shape))\n",
    "Normal = Gaussian # synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25139788 0.74841929 0.16438616]\n",
      "[[-3.36786391 -2.79262764 -2.97563054]\n",
      " [-2.79262764 -4.17533082 -3.16090872]\n",
      " [-2.97563054 -3.16090872 -3.75644909]]\n",
      "[0.25139788 0.74841929 0.16438616]\n",
      "[[-3.36786391 -2.79262764 -2.97563054]\n",
      " [-2.79262764 -4.17533082 -3.16090872]\n",
      " [-2.97563054 -3.16090872 -3.75644909]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.random.rand(10)\n",
    "X = np.random.rand(10,3)\n",
    "b = np.random.rand(3)\n",
    "\n",
    "g = Gaussian(lambda X,b:X@b) # g is g(y,X,b)\n",
    "func = lambda y,X,b: g(y,X,b)-0*np.sum(b**2)\n",
    "\n",
    "func2 = fwd.Diff(func, argno=2)\n",
    "f = func2.trace(y, X, b)\n",
    "j = func2.jacobian()\n",
    "print(j)\n",
    "h = func2.hessian()\n",
    "print(h)\n",
    "\n",
    "func2 = rev.Diff(func, argno=2)\n",
    "f = func2.trace(y, X, b)\n",
    "j = func2.jacobian()\n",
    "print(j)\n",
    "h = func2.hessian(gc=False)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25139788 0.74841929 0.16438616]\n",
      "[[-3.36786391 -2.79262764 -2.97563054]\n",
      " [-2.79262764 -4.17533082 -3.16090872]\n",
      " [-2.97563054 -3.16090872 -3.75644909]]\n"
     ]
    }
   ],
   "source": [
    "def linreg(y,X,b):\n",
    "    return -0.5*np.sum((y-X@b)**2)\n",
    "\n",
    "func2 = fwd.Diff(linreg, argno=2)\n",
    "f = func2.trace(y, X, b)\n",
    "j = func2.jacobian()\n",
    "print(j)\n",
    "h = func2.hessian()\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15127874, 0.62942399, 0.06286967])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(h, -j , rcond=None)[0]+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Errors.\n",
    "\n",
    "If $y$ is a 0,1 observation with expected value $\\mu$, then\n",
    "\n",
    "$$\n",
    "L = y\\log(\\mu) + (1-y)\\log(1-\\mu)\n",
    "$$\n",
    "\n",
    "The derivative of this wrt $\\mu$ is\n",
    "\n",
    "$$\n",
    "\\D{L}{\\mu} = \\frac{y}{\\mu}-\\frac{1-y}{1-\\mu} = \\frac{y(1-\\mu)-(1-y)\\mu}{\\mu(1-\\mu)}=\\frac{y-\\mu}{\\mu(1-\\mu)}\n",
    "$$\n",
    "\n",
    "Finally, \n",
    "\n",
    "$$\n",
    "E\\left(\\D{L}{\\mu}\\right)^2 = \\frac{E(y-\\mu)^2}{(\\mu(1-\\mu))^2}=\\frac{1}{\\mu(1-\\mu)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip = [0.001, 0.999]\n",
    "\n",
    "def logl(y,mu):\n",
    "    cmu = np.clip(mu, *clip)\n",
    "    return np.sum(y*np.log(cmu)+(1-y)*np.log(1-cmu))\n",
    "\n",
    "def dLdmu(y,mu):\n",
    "    cmu = np.clip(mu, *clip)\n",
    "    unclipped = cmu==mu\n",
    "    return unclipped*(y-cmu)/(cmu*(1-cmu))\n",
    "\n",
    "def varfunc(y,mu):\n",
    "    cmu = np.clip(mu, *clip)\n",
    "    unclipped = cmu==mu\n",
    "    return unclipped/(mu*(1-mu))\n",
    "\n",
    "Binomial = GLM_Builder(logl, dLdmu, varfunc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Errors.\n",
    "\n",
    "$$\n",
    "L \\propto y_i\\log{\\mu_i} -\\mu_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\D{L}{\\mu} = \\frac{y}{\\mu}-1 = \\frac{y-\\mu}{\\mu}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E\\left( \\D{L}{\\mu} \\right)^2 = \\frac{E((y-\\mu)^2)}{\\mu^2} = \\frac{\\mu}{\\mu^2} = \\frac{1}{\\mu}\n",
    "$$\n",
    "\n",
    "and $\\mu>0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logl(y,mu):\n",
    "    unclipped = mu>0\n",
    "    cmu = mu*unclipped\n",
    "    return np.sum(y*np.log(cmu)-cmu)\n",
    "    \n",
    "def dLdmu(y,mu):\n",
    "    unclipped = mu>0\n",
    "    return unclipped*(y-mu)/(1e-10+mu*unclipped)\n",
    "\n",
    "def varfunc(y,mu):\n",
    "    unclipped = mu>0\n",
    "    return unclipped/(1e-10+mu*unclipped)\n",
    "\n",
    "Poisson = GLM_Builder(logl, dLdmu, varfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Binomial.\n",
    "\n",
    "The probability of the number of failures before $1/\\alpha$ successes have occurred.\n",
    "\n",
    "Parameters are $\\alpha>0$ and $\\mu$, such that $\\alpha\\mu$ is positive.\n",
    "\n",
    "The likelihood is\n",
    "\n",
    "$$\n",
    "Pr(y|\\mu) = C \\left(\\frac{1}{1+\\alpha\\mu}\\right)^{\\frac{1}{\\alpha}}\n",
    "       \\left(\\frac{\\alpha\\mu}{1+\\alpha\\mu}\\right)^y\n",
    "$$\n",
    "\n",
    "where $C$ is a binomial coefficient. This is the parameterization chosen by statsmodels, because $E(y)=\\mu$.\n",
    "\n",
    "The log-likelihood is thus\n",
    "\n",
    "$$\n",
    "L(y, \\mu) = \\log{C} + \\frac{1}{\\alpha}\\log\\left(\\frac{1}{1+\\alpha\\mu}\\right)+y\\log\\left(\\frac{\\alpha\\mu}{1+\\alpha\\mu}\\right) \n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\D{L}{\\mu} = \\frac{y}{\\mu}-\\frac{1+\\alpha y}{1+\\alpha\\mu}=\\frac{y-\\mu}{\\mu+\\alpha\\mu^2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "E\\left(\\D{L}{\\mu}\\right)^2 = \\frac{1}{\\mu+\\alpha\\mu^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory to make negative binomial distributions:\n",
    "\n",
    "def NegativeBinomial(alpha):\n",
    "    # use as NegativeBinomial(alpha)(predictor)\n",
    "    \n",
    "    def logl(y,mu):\n",
    "        unclipped = mu>0\n",
    "        cmu = mu*unclipped\n",
    "        amu = alpha*cmu\n",
    "        return np.sum(-(1/alpha)*np.log(1+amu)+y*np.log(amu/(1+amu)))\n",
    "\n",
    "    def dLdmu(y,mu):\n",
    "        unclipped = mu>0\n",
    "        cmu = unclipped*mu\n",
    "        return unclipped*(y-mu)/(1e-10+cmu+alpha*cmu)\n",
    "\n",
    "    def varfunc(y,mu):\n",
    "        unclipped = mu>0\n",
    "        cmu = unclipped*mu\n",
    "        return 1.0/(1e-10+cmu+alpha*cmu)\n",
    "\n",
    "    return GLM_Builder(logl, dLdmu, varfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma, multinomial to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Functions.\n",
    "\n",
    "These functions give the derivatives used in GLMs. In GLM they are called mean functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@notrace\n",
    "def LinearMF(X,b):\n",
    "    # used with Gaussian\n",
    "    return X@b\n",
    "\n",
    "@fwd.register_deriv(LinearMF)\n",
    "def dLinearMF(X,b):\n",
    "    return X\n",
    "\n",
    "LinearGLM = Gaussian(LinearMF)\n",
    "\n",
    "@notrace\n",
    "def LogitMF(X,b):\n",
    "    # used with Binomial\n",
    "    nu = X@b\n",
    "    return 1/(1+np.exp(-nu))\n",
    "\n",
    "@fwd.register_deriv(LogitMF)\n",
    "def dLogitMF(X,b):\n",
    "    exp_nu = np.exp(-X@b)\n",
    "    return np.einsum('i,ij->ij', exp_nu/(1+exp_nu)**2, X)\n",
    "\n",
    "LogisticGLM = Binomial(LogitMF)\n",
    "    \n",
    "@notrace\n",
    "def ExpMF(X,b):\n",
    "    # used with Poisson\n",
    "    return np.exp(X@b)\n",
    "\n",
    "@fwd.register_deriv(ExpMF)\n",
    "def dExpMF(X,b):\n",
    "    return np.einsum('i,ij->ij', np.exp(X@b), X)\n",
    "\n",
    "PoissonGLM = Poisson(ExpMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7983891  2.03881795 2.95421102]\n",
      "[[-2.0250884  -1.51902008 -2.52993992]\n",
      " [-1.51902008 -3.07074464 -2.41104157]\n",
      " [-2.52993992 -2.41104157 -4.42575487]]\n",
      "[1.7983891  2.03881795 2.95421102]\n",
      "[[-2.0250884  -1.51902008 -2.52993992]\n",
      " [-1.51902008 -3.07074464 -2.41104157]\n",
      " [-2.52993992 -2.41104157 -4.42575487]]\n"
     ]
    }
   ],
   "source": [
    "y = np.random.rand(10)\n",
    "X = np.random.rand(10,3)\n",
    "b = 0*np.random.rand(3)\n",
    "\n",
    "func = lambda b: LinearGLM(y,X,b)-0*np.sum(b**2)\n",
    "\n",
    "func2 = fwd.Diff(func)\n",
    "f = func2.trace(b)\n",
    "j = func2.jacobian()\n",
    "print(j)\n",
    "h = func2.hessian()\n",
    "print(h)\n",
    "\n",
    "func2 = rev.Diff(func)\n",
    "f = func2.trace(b)\n",
    "j = func2.jacobian()\n",
    "print(j)\n",
    "h = func2.hessian(gc=False)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0250884 , 1.51902008, 2.52993992],\n",
       "       [1.51902008, 3.07074464, 2.41104157],\n",
       "       [2.52993992, 2.41104157, 4.42575487]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T@X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7983891 , 2.03881795, 2.95421102])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
